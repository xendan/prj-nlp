{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import random\n",
    "import pandas\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    def append_as_labeled(is_last, labeled_sent, sent):\n",
    "        is_first_lower = random.random() > 0.25\n",
    "        for i in range(len(sent) - 1):\n",
    "            labeled_sent.append([sent[i].lower() if i == 0 and is_first_lower else sent[i], i == len(sent) - 2])\n",
    "        if is_last:\n",
    "            labeled_sent.append([sent[-1], False])\n",
    "\n",
    "    def need_terminate(counter):\n",
    "        return (counter == 2 and random.random() > 0.7) or (counter == 3 and random.random() > 0.3) or counter > 3\n",
    "\n",
    "\n",
    "    df = pandas.read_csv('../../../../sources/ted-talks/transcripts.csv')\n",
    "\n",
    "    counter = 0\n",
    "    labeled_sentences = []\n",
    "    for index, row in df.iterrows():\n",
    "        if index < 400:\n",
    "            text = row['transcript'].replace('(Applause)',' ').replace('(Laughter)', ' ')\n",
    "            sentences = sent_tokenize(text)\n",
    "            counter = 0\n",
    "            labeled_sent = []\n",
    "            for sent_untoken in sentences:\n",
    "                sent = word_tokenize(sent_untoken)\n",
    "                counter += 1\n",
    "                is_last = need_terminate(counter)\n",
    "                append_as_labeled(is_last, labeled_sent, sent)\n",
    "                if is_last:\n",
    "                    counter = 0\n",
    "                    labeled_sent = []\n",
    "                    labeled_sentences.append(labeled_sent)\n",
    "\n",
    "    return train_test_split(labeled_sentences, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGrams:\n",
    "    \n",
    "    def __init__(self, use_lower = False):\n",
    "        self.use_lower = use_lower\n",
    "        self.trigrams = {}\n",
    "        self.bigrams = {}\n",
    "        self.unigrams = {}\n",
    "\n",
    "    def calculate(self):\n",
    "    \n",
    "        def get_word(words, i):\n",
    "            if i < 0:\n",
    "                return '<S>'\n",
    "            if i >= len(words):\n",
    "                return '</S>'\n",
    "            return words[i]\n",
    "        \n",
    "        def inc_key(key, data_dict):\n",
    "            val = 0\n",
    "            if key in data_dict:\n",
    "                val = data_dict[key]\n",
    "            data_dict[key] = val + 1\n",
    "\n",
    "        file = \"../../../../sources/nyt-comments/CommentsApril2018.csv\"\n",
    "        df = pandas.read_csv(file)[0:20000]\n",
    "    \n",
    "        for comment in df['commentBody']:\n",
    "            words = word_tokenize(comment)\n",
    "            for i in range(-2, len(words) + 1):\n",
    "                word1 = get_word(words, i)\n",
    "                word2 = get_word(words, i + 1)\n",
    "                word3 = get_word(words, i + 2)\n",
    "                inc_key(word1, self.unigrams)\n",
    "                inc_key((word1, word2), self.bigrams)\n",
    "                inc_key((word1, word2, word3), self.trigrams)\n",
    "\n",
    "                                \n",
    "ngrams = NGrams()\n",
    "ngrams.calculate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for sentence in data:\n",
    "        annotated = model.annotate(sentence)\n",
    "        for i in range(len(annotated)):\n",
    "            y_true.append(sentence[i][1])\n",
    "            y_pred.append(annotated[i][1])\n",
    "    \n",
    "    print(classification_report(y_true, y_pred))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model that calculate average number of words in sentence\n",
    "class AverageModel:\n",
    "    \n",
    "    def train(self, data):\n",
    "        def get_avg_sent_len(sent):\n",
    "            if (len(sent) == 0):\n",
    "                return 0\n",
    "            num = 0\n",
    "            for i in range(len(sent)):\n",
    "                if sent[i][1]:\n",
    "                    num += 1\n",
    "            return len(sent) / num\n",
    "        \n",
    "        total_len = 0\n",
    "        for sent in data:\n",
    "            total_len += get_avg_sent_len(sent)\n",
    "        \n",
    "        self.avg_len = int(total_len/len(data)) \n",
    "        \n",
    "        \n",
    "    def annotate(self, sentence):\n",
    "        annotated = []\n",
    "        for i in range(len(sentence)):\n",
    "            annotated.append([sentence[i][0], i != 0 and i % self.avg_len == 0])\n",
    "        \n",
    "        return annotated\n",
    "           \n",
    "                \n",
    "model_on_average = AverageModel()\n",
    "model_on_average.train(train)\n",
    "\n",
    "evaluate(model_on_average, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionModel:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vec = DictVectorizer()\n",
    "        self.logreg = LogisticRegression()\n",
    "    \n",
    "    def extract_features(self, i, sentence):\n",
    "        \n",
    "        def get_length():\n",
    "            length = 0\n",
    "            for j in range(i):\n",
    "                length += len(sentence[i])\n",
    "            return length\n",
    "        \n",
    "        def is_capital(word):\n",
    "            if len(word) == 0:\n",
    "                return False\n",
    "            return word[0].isupper()\n",
    "        \n",
    "        features = dict()\n",
    "        features[\"word\"] = sentence[i][0].lower()\n",
    "        features[\"next_is_capitalized\"] = is_capital(sentence[i + 1]) if i < len(sentence) - 1 else True\n",
    "        features[\"length_from_start\"] = get_length()\n",
    "        features[\"i\"] = i\n",
    "        #features[\"nex\"]\n",
    "        return features\n",
    "    \n",
    "    def train(self, data):\n",
    "        features, labels = [], []\n",
    "        \n",
    "        for sent in data:\n",
    "            for i in range(len(sent)):\n",
    "                features.append(self.extract_features(i, sent))\n",
    "                labels.append(sent[i][1])\n",
    "                    \n",
    "        self.logreg.fit(self.vec.fit_transform(features).toarray(), labels)\n",
    "    \n",
    "    def annotate(self, sentence):\n",
    "        annotated = []\n",
    "        for i in range(len(sentence)):\n",
    "            x = self.vec.transform(self.extract_features(i, sentence)).toarray()\n",
    "            predicted = self.logreg.predict(x)\n",
    "            annotated.append([sentence[i][0], predicted[0]])\n",
    "        return annotated\n",
    "        \n",
    "        \n",
    "log_reg_model = LogisticRegressionModel()\n",
    "log_reg_model.train(train)\n",
    "evaluate(log_reg_model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
