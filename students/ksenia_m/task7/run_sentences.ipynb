{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import brown\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    \n",
    "    def append_as_labeled(is_last, labeled_sent, sent):\n",
    "        is_first_lower = random.random() > 0.25\n",
    "        for i in range(len(sent) - 1):\n",
    "            labeled_sent.append([sent[i].lower() if i==0 and is_first_lower else sent[i], i == len(sent) - 2])\n",
    "        if is_last:\n",
    "            labeled_sent.append([sent[-1], False])\n",
    "    \n",
    "    def need_terminate(counter):\n",
    "        return (counter == 2 and random.random() > 0.7) or (counter == 3 and random.random() > 0.3) or counter > 3\n",
    "    \n",
    "    sentences = brown.sents(categories=['news', 'editorial', 'reviews', 'humor'])\n",
    "    counter = 0\n",
    "    labeled_sent = []\n",
    "    labeled_sentences = []\n",
    "    for sent in sentences:\n",
    "        counter += 1\n",
    "        is_last = need_terminate(counter)\n",
    "        append_as_labeled(is_last, labeled_sent, sent)\n",
    "        if is_last:\n",
    "            counter = 0\n",
    "            labeled_sent = []\n",
    "            labeled_sentences.append(labeled_sent)\n",
    "        \n",
    "    return train_test_split(labeled_sentences, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGrams:\n",
    "    \n",
    "    def __init__(self, use_lower = False):\n",
    "        self.use_lower = use_lower\n",
    "        self.trigrams = {}\n",
    "        self.bigrams = {}\n",
    "        self.unigrams = {}\n",
    "\n",
    "    def calculate(self):\n",
    "    \n",
    "        def get_word(words, i):\n",
    "            if i < 0:\n",
    "                return '<S>'\n",
    "            if i >= len(words):\n",
    "                return '</S>'\n",
    "            return words[i]\n",
    "        \n",
    "        def inc_key(key, data_dict):\n",
    "            val = 0\n",
    "            if key in data_dict:\n",
    "                val = data_dict[key]\n",
    "            data_dict[key] = val + 1\n",
    "            \n",
    "        sentences = brown.sents(categories = ['adventure', 'fiction', 'hobbies', 'mystery'])\n",
    "  \n",
    "        for sent in sentences:\n",
    "            for i in range(-2, len(sent) + 1):\n",
    "                word1 = get_word(sent, i)\n",
    "                word2 = get_word(sent, i + 1)\n",
    "                word3 = get_word(sent, i + 2)\n",
    "                inc_key(word1, self.unigrams)\n",
    "                inc_key((word1, word2), self.bigrams)\n",
    "                inc_key((word1, word2, word3), self.trigrams)\n",
    "    \n",
    "    # def get_log_prob_words_is_sentence(self, words):\n",
    "    #     \n",
    "    #     def get_log_prob_last_word(words, last_state):\n",
    "    #         log_sum = 0\n",
    "    #         for i in range(len(words) - 2):\n",
    "    #             log_sum += log_prob()\n",
    "    #         \n",
    "    #     return max(get_log_prob_last_word(words, True), get_log_prob_last_word(words, False)) \n",
    "    # \n",
    "\n",
    "                                \n",
    "ngrams = NGrams()\n",
    "ngrams.calculate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, data):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for sentence in data:\n",
    "        annotated = model.annotate(sentence)\n",
    "        for i in range(len(annotated)):\n",
    "            y_true.append(sentence[i][1])\n",
    "            y_pred.append(annotated[i][1])\n",
    "    \n",
    "    print(classification_report(y_true, y_pred, labels=['Is End', 'Not End']))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kseniia/anaconda/envs/py36/lib/python3.6/site-packages/numpy/lib/arraysetops.py:391: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n  mask &= (ar1 != a)\n/Users/kseniia/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n/Users/kseniia/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "Weights sum to zero, can't be normalized",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0834cf4e76ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mmodel_on_average\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_on_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#pessimistic_model = PessimisticModel()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-b6890bfc1631>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Is End'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Not End'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/kseniia/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits)\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[0;31m# compute averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlast_line_heading\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1428\u001b[0;31m     for v in (np.average(p, weights=s),\n\u001b[0m\u001b[1;32m   1429\u001b[0m               \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m               np.average(f1, weights=s)):\n",
      "\u001b[0;32m/Users/kseniia/anaconda/envs/py36/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned)\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscl\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m             raise ZeroDivisionError(\n\u001b[0;32m-> 1140\u001b[0;31m                 \"Weights sum to zero, can't be normalized\")\n\u001b[0m\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresult_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mscl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: Weights sum to zero, can't be normalized"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#model that calculate average number of words in sentence\n",
    "class AverageModel:\n",
    "    \n",
    "    def train(self, data):\n",
    "        def get_avg_sent_len(sent):\n",
    "            num = 0\n",
    "            for i in range(len(sent)):\n",
    "                if sent[i][1]:\n",
    "                    num += 1\n",
    "            return len(sent) / num\n",
    "        \n",
    "        total_len = 0\n",
    "        for sent in data:\n",
    "            total_len += get_avg_sent_len(sent)\n",
    "        \n",
    "        self.avg_len = int(total_len/len(data)) \n",
    "        \n",
    "        \n",
    "    def annotate(self, sentence):\n",
    "        annotated = []\n",
    "        print(self.avg_len)\n",
    "        for i in range(len(sentence)):\n",
    "            annotated.append([sentence[i][0], i != 0 and i % self.avg_len == 0])\n",
    "        \n",
    "        return annotated\n",
    "           \n",
    "                \n",
    "model_on_average = AverageModel()\n",
    "model_on_average.train(train)\n",
    "\n",
    "evaluate(model_on_average, test)\n",
    "\n",
    "#pessimistic_model = PessimisticModel()\n",
    "\n",
    "#evaluate(pessimistic_model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.7446808510638298 , recall = 0.08482792050412022 , f1 = 0.15230635335073978\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegressionModel:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vec = DictVectorizer()\n",
    "        self.logreg = LogisticRegression()\n",
    "    \n",
    "    def extract_features(self, i, sentence):\n",
    "        \n",
    "        def get_length():\n",
    "            length = 0\n",
    "            for j in range(i):\n",
    "                length += len(sentence[i])\n",
    "            return length\n",
    "        \n",
    "        def is_capital(word):\n",
    "            if len(word) == 0:\n",
    "                return False\n",
    "            return word[0].isupper()\n",
    "        \n",
    "        features = dict()\n",
    "        features[\"word\"] = sentence[i][0].lower()\n",
    "        #features[\"next_is_capitalized\"] = is_capital(sentence[i + 1]) if i < len(sentence) - 1 else True\n",
    "        #features[\"length_from_start\"] = get_length()\n",
    "        #features[\"i\"] = i\n",
    "        #features[\"nex\"]\n",
    "        return features\n",
    "    \n",
    "    def train(self, data):\n",
    "        features, labels = [], []\n",
    "        \n",
    "        for sent in data:\n",
    "            for i in range(len(sent)):\n",
    "                features.append(self.extract_features(i, sent))\n",
    "                labels.append(sent[i][1])\n",
    "                    \n",
    "        #print(\"DATA:\", features[0:20])\n",
    "        #print(self.vec.fit_transform(features).toarray()[0:10])\n",
    "        self.logreg.fit(self.vec.fit_transform(features).toarray(), labels)\n",
    "    \n",
    "    def annotate(self, sentence):\n",
    "        annotated = []\n",
    "        for i in range(len(sentence)):\n",
    "            x = self.vec.transform(self.extract_features(i, sentence)).toarray()\n",
    "            predicted = self.logreg.predict(x)\n",
    "            annotated.append([sentence[i][0], predicted[0]])\n",
    "        #print(annotated)\n",
    "        return annotated\n",
    "        \n",
    "        \n",
    "log_reg_model = LogisticRegressionModel()\n",
    "log_reg_model.train(train)\n",
    "evaluate(log_reg_model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
