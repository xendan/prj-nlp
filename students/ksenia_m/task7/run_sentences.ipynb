{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import brown\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    \n",
    "    def append_as_labeled(is_last, labeled_sent, sent):\n",
    "        is_first_lower = random.random() > 0.25\n",
    "        for i in range(len(sent) - 1):\n",
    "            labeled_sent.append([sent[i].lower() if i==0 and is_first_lower else sent[i], i == len(sent) - 2])\n",
    "        if is_last:\n",
    "            labeled_sent.append([sent[-1], False])\n",
    "    \n",
    "    def need_terminate(counter):\n",
    "        return (counter == 2 and random.random() > 0.7) or (counter == 3 and random.random() > 0.3) or counter > 3\n",
    "    \n",
    "    sentences = brown.sents(categories=['news', 'editorial', 'reviews', 'humor'])\n",
    "    counter = 0\n",
    "    labeled_sent = []\n",
    "    labeled_sentences = []\n",
    "    for sent in sentences:\n",
    "        counter += 1\n",
    "        is_last = need_terminate(counter)\n",
    "        append_as_labeled(is_last, labeled_sent, sent)\n",
    "        if is_last:\n",
    "            counter = 0\n",
    "            labeled_sent = []\n",
    "            labeled_sentences.append(labeled_sent)\n",
    "        \n",
    "    return train_test_split(labeled_sentences, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGrams:\n",
    "    \n",
    "    def __init__(self, use_lower = False):\n",
    "        self.use_lower = use_lower\n",
    "        self.trigrams = {}\n",
    "        self.bigrams = {}\n",
    "        self.unigrams = {}\n",
    "\n",
    "    def calculate(self):\n",
    "    \n",
    "        def get_word(words, i):\n",
    "            if i < 0:\n",
    "                return '<S>'\n",
    "            if i >= len(words):\n",
    "                return '</S>'\n",
    "            return words[i]\n",
    "        \n",
    "        def inc_key(key, data_dict):\n",
    "            val = 0\n",
    "            if key in data_dict:\n",
    "                val = data_dict[key]\n",
    "            data_dict[key] = val + 1\n",
    "            \n",
    "        sentences = brown.sents(categories = ['adventure', 'fiction', 'hobbies', 'mystery'])\n",
    "  \n",
    "        for sent in sentences:\n",
    "            for i in range(-2, len(sent) + 1):\n",
    "                word1 = get_word(sent, i)\n",
    "                word2 = get_word(sent, i + 1)\n",
    "                word3 = get_word(sent, i + 2)\n",
    "                inc_key(word1, self.unigrams)\n",
    "                inc_key((word1, word2), self.bigrams)\n",
    "                inc_key((word1, word2, word3), self.trigrams)\n",
    "    \n",
    "    # def get_log_prob_words_is_sentence(self, words):\n",
    "    #     \n",
    "    #     def get_log_prob_last_word(words, last_state):\n",
    "    #         log_sum = 0\n",
    "    #         for i in range(len(words) - 2):\n",
    "    #             log_sum += log_prob()\n",
    "    #         \n",
    "    #     return max(get_log_prob_last_word(words, True), get_log_prob_last_word(words, False)) \n",
    "    # \n",
    "\n",
    "                                \n",
    "ngrams = NGrams()\n",
    "ngrams.calculate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, data):\n",
    "    results = {(True, True) : 0, (False, False) : 0, (True, False) : 0, (False, True) : 0}\n",
    "    for sentence in data:\n",
    "        annotated = model.annotate(sentence)\n",
    "        for i in range(len(annotated)):\n",
    "            results[(sentence[i][1], annotated[i][1])] = results[(sentence[i][1], annotated[i][1])] + 1\n",
    "    \n",
    "    precision = results[(True, True)] / (results[(True, True)] + results[(False, True)])\n",
    "    recall = results[(True, True)] / (results[(True, True)] + results[(True, False)])\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    print(\"For \", type(model).__name__)\n",
    "    print(\"precision =\", precision, \", recall =\", recall, \", f1 =\", f1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  AverageModel\nprecision = 0.06451612903225806 , recall = 0.010664081434803683 , f1 = 0.018302828618968384\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-7b8926b57a5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mpessimistic_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPessimisticModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpessimistic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-accb9b315e89>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#model that calculate average number of words in sentence\n",
    "class AverageModel:\n",
    "    \n",
    "    def train(self, data):\n",
    "        total_len = 0\n",
    "        for sent in data:\n",
    "            total_len += len(sent)\n",
    "        \n",
    "        self.avg_len = int(total_len/len(data)) \n",
    "        \n",
    "        \n",
    "    def annotate(self, sentence):\n",
    "        annotated = []\n",
    "        \n",
    "        for i in range(len(sentence)):\n",
    "            annotated.append([sentence[i][0], i != 0 and i % self.avg_len == 0])\n",
    "        \n",
    "        return annotated\n",
    "           \n",
    "class PessimisticModel:\n",
    "    \n",
    "    def annotate(self, sentence):\n",
    "        annotated = []\n",
    "        for i in range(len(sentence)):\n",
    "            annotated.append([sentence[i][0], False])\n",
    "        \n",
    "        return annotated\n",
    "                \n",
    "model_on_average = AverageModel()\n",
    "model_on_average.train(train)\n",
    "\n",
    "evaluate(model_on_average, test)\n",
    "\n",
    "pessimistic_model = PessimisticModel()\n",
    "\n",
    "evaluate(pessimistic_model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.7446808510638298 , recall = 0.08482792050412022 , f1 = 0.15230635335073978\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegressionModel:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vec = DictVectorizer()\n",
    "        self.logreg = LogisticRegression()\n",
    "    \n",
    "    def extract_features(self, i, sentence):\n",
    "        \n",
    "        def get_length():\n",
    "            length = 0\n",
    "            for j in range(i):\n",
    "                length += len(sentence[i])\n",
    "            return length\n",
    "        \n",
    "        def is_capital(word):\n",
    "            if len(word) == 0:\n",
    "                return False\n",
    "            return word[0].isupper()\n",
    "        \n",
    "        features = dict()\n",
    "        features[\"word\"] = sentence[i][0].lower()\n",
    "        #features[\"next_is_capitalized\"] = is_capital(sentence[i + 1]) if i < len(sentence) - 1 else True\n",
    "        #features[\"length_from_start\"] = get_length()\n",
    "        #features[\"i\"] = i\n",
    "        #features[\"nex\"]\n",
    "        return features\n",
    "    \n",
    "    def train(self, data):\n",
    "        features, labels = [], []\n",
    "        \n",
    "        for sent in data:\n",
    "            for i in range(len(sent)):\n",
    "                features.append(self.extract_features(i, sent))\n",
    "                labels.append(sent[i][1])\n",
    "                    \n",
    "        #print(\"DATA:\", features[0:20])\n",
    "        #print(self.vec.fit_transform(features).toarray()[0:10])\n",
    "        self.logreg.fit(self.vec.fit_transform(features).toarray(), labels)\n",
    "    \n",
    "    def annotate(self, sentence):\n",
    "        annotated = []\n",
    "        for i in range(len(sentence)):\n",
    "            x = self.vec.transform(self.extract_features(i, sentence)).toarray()\n",
    "            predicted = self.logreg.predict(x)\n",
    "            annotated.append([sentence[i][0], predicted[0]])\n",
    "        #print(annotated)\n",
    "        return annotated\n",
    "        \n",
    "        \n",
    "log_reg_model = LogisticRegressionModel()\n",
    "log_reg_model.train(train)\n",
    "evaluate(log_reg_model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
